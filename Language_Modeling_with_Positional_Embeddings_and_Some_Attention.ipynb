{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiaoYunTing/Text-Analytics/blob/main/Language_Modeling_with_Positional_Embeddings_and_Some_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Model with Positional Embeddings and Elementary 'Attention'\n",
        "\n",
        "We will expand the previous simple language model by adding  positional embeddings and simplified attention. As before we will use the Jane Austen novel 'Sense and Sensibility' to train the language model.\n",
        "\n",
        "First we go through the preliminaries to get the model ready to train."
      ],
      "metadata": {
        "id": "3xvfc50c3q5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')  # Make sure the Gutenberg corpus is downloaded\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "mUhVAAMI4VXR",
        "outputId": "7992fb94-bf84-4e10-eb3b-fb836c6a6390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminaries"
      ],
      "metadata": {
        "id": "Gly10Lq0awtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load \"Sense and Sensibility\" text\n",
        "sas = gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Print the first 500 characters of \"Sense and Sensibility\"\n",
        "print(sas[:500])\n"
      ],
      "metadata": {
        "id": "ZWmkgwQIWnu1",
        "outputId": "91486c23-04b0-418e-b529-5c6f0120e1c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sense and Sensibility by Jane Austen 1811]\n",
            "\n",
            "CHAPTER 1\n",
            "\n",
            "\n",
            "The family of Dashwood had long been settled in Sussex.\n",
            "Their estate was large, and their residence was at Norland Park,\n",
            "in the centre of their property, where, for many generations,\n",
            "they had lived in so respectable a manner as to engage\n",
            "the general good opinion of their surrounding acquaintance.\n",
            "The late owner of this estate was a single man, who lived\n",
            "to a very advanced age, and who for many years of his life,\n",
            "had a constant companion an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "We will use the `gensim` library, which provides straightforward implementations of `word2vec`, to create word embeddings using the CBOW model. To keep it simple, we will create embeddings of size 5."
      ],
      "metadata": {
        "id": "WcvwuYiDf-SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(sas)\n",
        "\n",
        "# Organize the tokens into sentences, Word2Vec needs data in format of list of lists of tokens\n",
        "sentences = [tokens[i:i+100] for i in range(0, len(tokens), 100)]\n"
      ],
      "metadata": {
        "id": "52rocFVboYXz",
        "outputId": "0fd0bb8f-4589-4764-cd04-5dc6593474f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CBOW model\n",
        "model = Word2Vec(sentences, vector_size=5, window=5, min_count=1, sg=0)  # sg=0 specifies CBOW\n"
      ],
      "metadata": {
        "id": "HO3VOV1Coa4i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizer\n",
        "Once we get the embeddings, we will store the words (tokens), token ids, and embeddings in a dataframe.\n",
        "\n",
        "Note that the number of distinct words (tokens) is 7111. This is the size of our vocabulary."
      ],
      "metadata": {
        "id": "it6haWBx9wI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create a DataFrame to store word, token_id, and embedding\n",
        "data = {\n",
        "    'word': [],\n",
        "    'token_id': [],\n",
        "    'embedding': []\n",
        "}\n",
        "\n",
        "for idx, word in enumerate(model.wv.index_to_key):\n",
        "    data['word'].append(word)\n",
        "    data['token_id'].append(idx)\n",
        "    data['embedding'].append(model.wv[word].tolist())  # convert numpy array to list for easier handling in DataFrame\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ut2FlOwE90d3",
        "outputId": "f0060746-6edc-4c7e-db29-7ea0d20df73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              word  token_id  \\\n",
            "0                ,         0   \n",
            "1               to         1   \n",
            "2                .         2   \n",
            "3              the         3   \n",
            "4               of         4   \n",
            "...            ...       ...   \n",
            "7106        other.      7106   \n",
            "7107   incessantly      7107   \n",
            "7108  exclamations      7108   \n",
            "7109     hartshorn      7109   \n",
            "7110       utility      7110   \n",
            "\n",
            "                                              embedding  \n",
            "0     [2.620171546936035, 1.8446730375289917, 3.9585...  \n",
            "1     [1.5974335670471191, 1.9683901071548462, 4.129...  \n",
            "2     [5.972449779510498, 2.2278716564178467, 3.3245...  \n",
            "3     [-0.6165288686752319, 1.8960069417953491, 4.91...  \n",
            "4     [1.085107445716858, 1.1664472818374634, 5.4929...  \n",
            "...                                                 ...  \n",
            "7106  [0.1374606043100357, 0.206727996468544, 0.1856...  \n",
            "7107  [-0.14069943130016327, -0.0452144593000412, -0...  \n",
            "7108  [-0.05511047691106796, 0.053439486771821976, -...  \n",
            "7109  [0.03255762904882431, -0.15563656389713287, 0....  \n",
            "7110  [0.05168135091662407, 0.06414742022752762, -0....  \n",
            "\n",
            "[7111 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Positional Embeddings\n",
        "Positional embeddings are used primarily in models like the Transformer to incorporate information about the position of tokens in the input sequence into the model. The idea is to add a vector to each token's embedding that represents its position in the sequence, ensuring that the order of tokens contributes to the model's understanding.\n",
        "\n",
        "In the original Transformer architecture, positional embeddings are computed using sine and cosine functions of different frequencies:\n",
        "\n",
        "$$\n",
        "\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\\\\n",
        "\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "$pos$ is the position of the token in the sequence.\n",
        "\n",
        "$i$ is the dimension index.\n",
        "\n",
        "$\n",
        "d_{model}\n",
        "$ is the dimensionality of the token embeddings.\n",
        "\n",
        "This formula helps the model to differentiate positions by providing a unique signal for each position, and the repeating patterns allow the model to generalize to sequence lengths that it has not seen before."
      ],
      "metadata": {
        "id": "QSzAD9ABFT9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_embeddings(sequence_length, embedding_dim):\n",
        "    positional_embeddings = np.zeros((sequence_length, embedding_dim))\n",
        "    for pos in range(sequence_length):\n",
        "        for i in range(embedding_dim):\n",
        "            if i % 2 == 0:\n",
        "                positional_embeddings[pos, i] = np.sin(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "            else:\n",
        "                positional_embeddings[pos, i] = np.cos(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "    return positional_embeddings\n"
      ],
      "metadata": {
        "id": "9Mu322EWn3oT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Data\n",
        "Our main objective is to predict the next word (token) based on the previous 5 words (tokens). Thus, our context length is 5.\n",
        "\n",
        "We will prepare the training data such that inputs are 5 consecutive words (token) and the output to be predicted is the 6th word (token). If the input has less than 5 words (tokens), we will pad it with \\<pad>.\n",
        "\n",
        "Add stuff about positional embeddings"
      ],
      "metadata": {
        "id": "sem7F-I3_qLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_training_data(sentences, model_wv, window_size=5):\n",
        "    embedding_dim = model_wv.vector_size\n",
        "    X, y = [], []\n",
        "    sequence_texts = []  # For storing the actual sequences of words\n",
        "    next_words = []  # For storing the actual next word\n",
        "    positional_embeddings = get_positional_embeddings(window_size, embedding_dim)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Embed words using the Word2Vec model\n",
        "        embedded_sentence = [model_wv[word] for word in sentence if word in model_wv]\n",
        "        word_sentence = [word for word in sentence if word in model_wv]  # Keep the actual words for viewing\n",
        "\n",
        "        # Create sequences\n",
        "        for i in range(len(embedded_sentence)):\n",
        "            end_ix = i + window_size\n",
        "            if end_ix >= len(embedded_sentence):\n",
        "                break\n",
        "            seq_x, seq_y = embedded_sentence[i:end_ix], embedded_sentence[end_ix]\n",
        "            seq_text, next_word = word_sentence[i:end_ix], word_sentence[end_ix]\n",
        "\n",
        "            # Pad sequence if necessary\n",
        "            padding_length = window_size - len(seq_x)\n",
        "            seq_x += [np.zeros(embedding_dim)] * padding_length\n",
        "            seq_text += ['<pad>'] * padding_length\n",
        "\n",
        "            # Add positional embeddings\n",
        "            modified_seq_x = np.array(seq_x) + positional_embeddings[:len(seq_x)]\n",
        "\n",
        "            X.append(modified_seq_x.flatten())\n",
        "            y.append(seq_y)\n",
        "            sequence_texts.append(' '.join(seq_text))\n",
        "            next_words.append(next_word)\n",
        "\n",
        "    return np.array(X), np.array(y), sequence_texts, next_words\n"
      ],
      "metadata": {
        "id": "j9KWNVeVn5zD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, train_sequences, train_next_words = generate_training_data(sentences, model.wv)"
      ],
      "metadata": {
        "id": "X7jMQJ-Fn5wC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "train_df = pd.DataFrame({\n",
        "    'Sequence': train_sequences,\n",
        "    'Next Word': train_next_words,\n",
        "    'X_train (Flattened Embeddings)': list(X_train),\n",
        "    'y_train (Embedding)': list(y_train)})"
      ],
      "metadata": {
        "id": "sED1oOh7n5uT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "RgU0IPbZn5qD",
        "outputId": "b6f47d1d-a135-47b7-e933-df9c1bfb5530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Sequence Next Word  \\\n",
              "0       [ Sense and Sensibility by      Jane   \n",
              "1    Sense and Sensibility by Jane    Austen   \n",
              "2   and Sensibility by Jane Austen      1811   \n",
              "3  Sensibility by Jane Austen 1811         ]   \n",
              "4            by Jane Austen 1811 ]   CHAPTER   \n",
              "\n",
              "                      X_train (Flattened Embeddings)  \\\n",
              "0  [-0.05941634625196457, 1.114052876830101, 0.03...   \n",
              "1  [0.21857543289661407, 1.2241209894418716, 0.20...   \n",
              "2  [0.6344841718673706, 3.568253993988037, 4.1085...   \n",
              "3  [0.16189710795879364, 0.9867487233132124, 0.21...   \n",
              "4  [0.7093105912208557, 3.1029014587402344, 4.897...   \n",
              "\n",
              "                                 y_train (Embedding)  \n",
              "0  [-0.15103865, -0.13794702, 0.23254584, -0.1802...  \n",
              "1  [0.2371671, -0.035784535, -0.07405981, -0.2451...  \n",
              "2  [0.13259846, -0.08520899, 0.16910738, -0.24759...  \n",
              "3  [0.2779051, 0.39517418, 0.32963225, -0.3987266...  \n",
              "4  [1.2434641, 1.530862, 2.7463584, -1.8938402, -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-613b96b8-7e2d-47c1-b443-63130781ba12\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sequence</th>\n",
              "      <th>Next Word</th>\n",
              "      <th>X_train (Flattened Embeddings)</th>\n",
              "      <th>y_train (Embedding)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ Sense and Sensibility by</td>\n",
              "      <td>Jane</td>\n",
              "      <td>[-0.05941634625196457, 1.114052876830101, 0.03...</td>\n",
              "      <td>[-0.15103865, -0.13794702, 0.23254584, -0.1802...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sense and Sensibility by Jane</td>\n",
              "      <td>Austen</td>\n",
              "      <td>[0.21857543289661407, 1.2241209894418716, 0.20...</td>\n",
              "      <td>[0.2371671, -0.035784535, -0.07405981, -0.2451...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and Sensibility by Jane Austen</td>\n",
              "      <td>1811</td>\n",
              "      <td>[0.6344841718673706, 3.568253993988037, 4.1085...</td>\n",
              "      <td>[0.13259846, -0.08520899, 0.16910738, -0.24759...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sensibility by Jane Austen 1811</td>\n",
              "      <td>]</td>\n",
              "      <td>[0.16189710795879364, 0.9867487233132124, 0.21...</td>\n",
              "      <td>[0.2779051, 0.39517418, 0.32963225, -0.3987266...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>by Jane Austen 1811 ]</td>\n",
              "      <td>CHAPTER</td>\n",
              "      <td>[0.7093105912208557, 3.1029014587402344, 4.897...</td>\n",
              "      <td>[1.2434641, 1.530862, 2.7463584, -1.8938402, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-613b96b8-7e2d-47c1-b443-63130781ba12')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-613b96b8-7e2d-47c1-b443-63130781ba12 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-613b96b8-7e2d-47c1-b443-63130781ba12');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-53dfe799-47ad-464e-b47e-41b891b78680\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53dfe799-47ad-464e-b47e-41b891b78680')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-53dfe799-47ad-464e-b47e-41b891b78680 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training dataset has 134,306 data points. Each data point has 6 words (tokens), and thus the total number of words (tokens) for training is 134,306 * 6 = 805,836."
      ],
      "metadata": {
        "id": "oimxyIyo7DU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape\n",
        "# train_df.head()"
      ],
      "metadata": {
        "id": "0A9NtzIFA1i5",
        "outputId": "d6519132-91e4-469f-b144-391a43f2509d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(134306, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network Design\n",
        "This time we will add two more layers to have the following structure.\n",
        "\n",
        "1. Input layer: 25 nodes (5 words(tokens) with embedding size of 5 for each).\n",
        "2. Attention-Like Layer: 25 nodes. A dense layer can serve as a basic form of attention by processing input features, although true attention mechanisms (like those in transformers) are more complex.\n",
        "3. Hidden layer: 10 nodes.\n",
        "4. Output layer: 5 nodes (for the predicted next word (token)).\n",
        "_________________________________________________________________\n",
        "\n",
        "![My Image](https://drive.google.com/uc?export=view&id=1lcTDUrhvAgz85zOgFPK2Nzdeiqou3jlP)\n"
      ],
      "metadata": {
        "id": "QqdrzAMjBgBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, Add, Input, Lambda\n",
        "import tensorflow as tf\n",
        "from keras import Model\n"
      ],
      "metadata": {
        "id": "5V9KKsM5wb9m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Activation, InputLayer, Lambda\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "\n",
        "def build_attention_model(input_dim, attention_nodes, hidden_nodes, output_dim):\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(input_dim,)),  # Input layer specifying input shape\n",
        "        Dense(attention_nodes),  # Attention-like layer without activation\n",
        "        Activation('softmax'),  # Apply softmax to simulate attention properly\n",
        "        Dense(hidden_nodes, activation='relu'),  # Hidden layer\n",
        "        Dense(output_dim, activation='linear')  # Output layer for the embeddings\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and summarize the model\n",
        "input_dim = 25  # Total input dimensions (5 words * 5 dimensions each)\n",
        "attention_nodes = 25  # Nodes in the attention layer\n",
        "hidden_nodes = 10  # Nodes in the hidden layer\n",
        "output_dim = 5  # Dimensions of the output embeddings\n",
        "\n",
        "attention_model = build_attention_model(input_dim, attention_nodes, hidden_nodes, output_dim)\n",
        "attention_model.summary()\n"
      ],
      "metadata": {
        "id": "EmKdFPYnwzaG",
        "outputId": "cf0847d0-77de-4a4b-f67f-10e29d16253a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 25)                650       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 25)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                260       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 55        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 965 (3.77 KB)\n",
            "Trainable params: 965 (3.77 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train the model\n",
        "**Takes a long time - about an hour on my machine**\n",
        "\n",
        "**I have saved the trained model (`at_model.h5`). To run the trained model you simply have to load the saved model and run it. **"
      ],
      "metadata": {
        "id": "VP4CbZBt7Nfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Train the model with the training data\n",
        "attention_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "attention_model.save('at_model.h5')\n"
      ],
      "metadata": {
        "id": "hhPL0D2cw66U",
        "outputId": "9c19b32c-6322-4fa7-85cc-c6d7f5d1f601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4198/4198 [==============================] - 9s 2ms/step - loss: 1.8571 - accuracy: 0.7357\n",
            "Epoch 2/50\n",
            "4198/4198 [==============================] - 7s 2ms/step - loss: 1.6821 - accuracy: 0.7394\n",
            "Epoch 3/50\n",
            "1466/4198 [=========>....................] - ETA: 6s - loss: 1.6800 - accuracy: 0.7386"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-09a5fa797068>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example: Train the model with the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'at_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "at_model = load_model('at_model.h5')\n"
      ],
      "metadata": {
        "id": "hOk4g9Scwk1W"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next word prediction"
      ],
      "metadata": {
        "id": "s6mvlp9uzGQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Assuming `model` and `at_model` are loaded elsewhere and correctly:\n",
        "word2vec_model = model  # Assuming `model` is your Word2Vec model loaded correctly\n",
        "at_model = load_model('at_model.h5')  # Ensure this is loaded correctly as well\n",
        "\n",
        "def get_positional_embeddings(sequence_length, embedding_dim):\n",
        "    positional_embeddings = np.zeros((sequence_length, embedding_dim))\n",
        "    for pos in range(sequence_length):\n",
        "        for i in range(embedding_dim):\n",
        "            if i % 2 == 0:\n",
        "                positional_embeddings[pos, i] = np.sin(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "            else:\n",
        "                positional_embeddings[pos, i] = np.cos(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "    return positional_embeddings\n",
        "\n",
        "def preprocess_input_text(text, word2vec_model, sequence_length=5):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    embeddings = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
        "    embeddings += [np.zeros(word2vec_model.vector_size)] * (sequence_length - len(embeddings))\n",
        "    positional_embeddings = get_positional_embeddings(sequence_length, word2vec_model.vector_size)\n",
        "    modified_embeddings = [embed + pos_embed for embed, pos_embed in zip(embeddings, positional_embeddings)]\n",
        "    return np.concatenate(modified_embeddings)\n",
        "\n",
        "def find_top_five_words(embedding, word2vec_model):\n",
        "    word_distances = [(word, cosine(embedding, word2vec_model.wv[word])) for word in word2vec_model.wv.index_to_key]\n",
        "    word_distances.sort(key=lambda x: x[1])\n",
        "    return [word for word, _ in word_distances[:5]]\n",
        "\n",
        "def next_word_prediction(input_text, neural_model, word2vec_model):\n",
        "    input_vec = preprocess_input_text(input_text, word2vec_model)\n",
        "    predicted_embedding = neural_model.predict(np.array([input_vec]))[0]\n",
        "    return find_top_five_words(predicted_embedding, word2vec_model)\n"
      ],
      "metadata": {
        "id": "rskMV7O83ja1",
        "outputId": "55bcbee6-3eb5-41a6-d738-db62e5f12afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_text = \"they have\" # @param {type:\"string\"}\n",
        "predicted_words = next_word_prediction(input_text, at_model, word2vec_model)"
      ],
      "metadata": {
        "id": "Mpt3j5mS3jX-",
        "outputId": "bf19c688-21d2-450f-f138-7277e642e1a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_words"
      ],
      "metadata": {
        "id": "n1kaoea33jT-",
        "outputId": "df63be28-a185-4e32-944c-03e538852315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['make', 'creature', 'receipt', 'intended', 'back']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation"
      ],
      "metadata": {
        "id": "j2ikOoLH5bBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_words(embedding, word2vec_model, top_n=20):\n",
        "    word_distances = [(word, cosine(embedding, word2vec_model.wv[word])) for word in word2vec_model.wv.index_to_key]\n",
        "    word_distances.sort(key=lambda x: x[1])\n",
        "    return word_distances[:top_n]\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_text(initial_text, model, word2vec_model, num_words):\n",
        "    generated_words = word_tokenize(initial_text.lower())\n",
        "    sequence_length = 5  # Assume the same sequence length as used in training\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        # Preprocess the current text to input format\n",
        "        input_vec = preprocess_input_text(' '.join(generated_words[-sequence_length:]), word2vec_model, sequence_length)\n",
        "        predicted_embedding = model.predict(np.array([input_vec]))[0]\n",
        "\n",
        "        # Get top 20 predictions and randomly choose from them\n",
        "        top_predictions = find_top_words(predicted_embedding, word2vec_model, top_n=20)\n",
        "        next_word = random.choice(top_predictions)[0]\n",
        "\n",
        "        # Append the randomly chosen word\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "KkbMoq_h3jPu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "initial_text = \"they had lived in so\" # @param {type:\"string\"}\n",
        "num_words_to_generate = 50 # @param {type:\"integer\"}\n",
        "generated_text = generate_text(initial_text, at_model, word2vec_model, num_words_to_generate)"
      ],
      "metadata": {
        "id": "EXb_priF3jM2",
        "outputId": "d43fea61-a8c5-4fee-a77e-8854249f1c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "0j4CS9fo3jJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a slightly more advanced language model than the previous model but still performs rather poorly."
      ],
      "metadata": {
        "id": "02HvMRP2-vSE"
      }
    }
  ]
}